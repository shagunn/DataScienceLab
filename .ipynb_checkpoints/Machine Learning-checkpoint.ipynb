{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the matplotlib plotted graph within notebook lines.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# pandas:Data framework library for Python\n",
    "# sklearn: Library to perform machine learning tasks\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import re\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn \n",
    "import sklearn.datasets\n",
    "import sklearn.metrics as metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract data\n",
    "#!tar -xf 20news-19997.tar.gz -C .\n",
    "!tar -xf 20news-19997.tar\n",
    "\n",
    "#!gzip -d -k 20news-19997.tar.gz\n",
    "\n",
    "#!tar -xf 20news-19997.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34malt.atheism\u001b[m\u001b[m              \u001b[34mrec.autos\u001b[m\u001b[m                \u001b[34msci.space\u001b[m\u001b[m\r\n",
      "\u001b[34mcomp.graphics\u001b[m\u001b[m            \u001b[34mrec.motorcycles\u001b[m\u001b[m          \u001b[34msoc.religion.christian\u001b[m\u001b[m\r\n",
      "\u001b[34mcomp.os.ms-windows.misc\u001b[m\u001b[m  \u001b[34mrec.sport.baseball\u001b[m\u001b[m       \u001b[34mtalk.politics.guns\u001b[m\u001b[m\r\n",
      "\u001b[34mcomp.sys.ibm.pc.hardware\u001b[m\u001b[m \u001b[34mrec.sport.hockey\u001b[m\u001b[m         \u001b[34mtalk.politics.mideast\u001b[m\u001b[m\r\n",
      "\u001b[34mcomp.sys.mac.hardware\u001b[m\u001b[m    \u001b[34msci.crypt\u001b[m\u001b[m                \u001b[34mtalk.politics.misc\u001b[m\u001b[m\r\n",
      "\u001b[34mcomp.windows.x\u001b[m\u001b[m           \u001b[34msci.electronics\u001b[m\u001b[m          \u001b[34mtalk.religion.misc\u001b[m\u001b[m\r\n",
      "\u001b[34mmisc.forsale\u001b[m\u001b[m             \u001b[34msci.med\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "# display newsgroups directories\n",
    "!ls 20_newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Data Preprocessing and Initial Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) How did you choose to construct your features? I.e., please specify the following: (i) the feature encoding method (hint: unigram, bigram, stem?, lowercase?), (ii) the method for ranking features, and (iii) your choice of how many features to select. Explain your rationale for each choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Feature construction:\n",
    " * feature encoding: unigram, lemmatize, remove stop wrods, term frequency\n",
    " * features were ranked by mutual information\n",
    " * top 1000 most frequent features selected for MI (reasonable number given computational limits), then 250 features selected. Initially 500, but minimal accuracy tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write function dataPreprocessor:  data preprocessing settings (number of features, feature selection method, encoding method, and size of data) as input, and it should output a pandas Dataframe that contains preprocessed data where columns are features (except the last one, which is the target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataPreprocessor(k, fs, encode, directorydf, size=None):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        k: int. number of features to use\n",
    "        fs: string. From ['tf', 'mi']\n",
    "        encode: string. From ['tf', 'boolean']\n",
    "        drectorydf: Dataframe. It is given, see above support functions\n",
    "        size: int. Sample size. Default should be the data size.\n",
    "    OUTPUT\n",
    "        data: Dataframe. preprocessed data\n",
    "    \n",
    "    ps: 'tf' means term frequency, 'mi' means mutual information\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "    data = None\n",
    "    \n",
    "    #if fs == 'mi' and encode == 'tf':\n",
    "    if fs == 'mi':\n",
    "\n",
    "\n",
    "        \n",
    "        #handle size\n",
    "        wnLemm = WordNetLemmatizer()\n",
    "        unfiltered_counter = Counter()\n",
    "        for rownum, row in enumerate(directorydf.itertuples()):\n",
    "            with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile: \n",
    "                unfiltered_counter.update([wnLemm.lemmatize(word.lower(), 'v') for word in re.findall(r'\\w+', myfile.read())]) \n",
    "                #for word in re.findall(r'\\w+', myfile.read()):\n",
    "                #    if nltk.corpus.wordnet.synsets(word) and len(word) > 2:\n",
    "                #        unfiltered_counter.update([wnLemm.lemmatize(word.lower())])\n",
    "        \n",
    "        print(len(unfiltered_counter))\n",
    "        \n",
    "        #my_k = k + 200\n",
    "        my_k = 1000\n",
    "        \n",
    "        if len(unfiltered_counter) < my_k:\n",
    "            my_k = len(unfiltered_counter)\n",
    "            \n",
    "        filtered_set = {}\n",
    "        for word, count in dict(unfiltered_counter.most_common(my_k)).items():\n",
    "            #print(word)\n",
    "            if word.isalpha() and word not in stopwords.words('english') and len(word) > 2:\n",
    "                filtered_set[word] = count\n",
    "                #print(word)\n",
    "        \n",
    "        #print(len(filtered_set))\n",
    "        \n",
    "        counter = Counter(dict(filtered_set))\n",
    "        topk = counter.most_common()\n",
    "        \n",
    "        \n",
    "        \n",
    "        np = []\n",
    "        # now that we have top k words, count the term frequecy of these words in individual file\n",
    "        for rownum, row in enumerate(directorydf.itertuples()):\n",
    "            with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "                tempCounter = Counter([word for word in re.findall(r'\\w+', myfile.read())])\n",
    "                # if the word appears in the doc, then 1, else \n",
    "                if encode == 'tf':\n",
    "                    topkinDoc = [tempCounter[word] for (word,wordCount) in topk]\n",
    "                    # create a list for top k words with encoded target and its label\n",
    "                    np.append(topkinDoc+[label_target(row.Directories)]+[row.Directories])\n",
    "                else:\n",
    "                    topkinDoc = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "                    np.append(topkinDoc+[label_target(row.Directories)]+[row.Directories])\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        newsgroupdf = pd.DataFrame(np)\n",
    "        dfName = []\n",
    "        for c in topk:\n",
    "            dfName.append(c[0])\n",
    "            \n",
    "        newsgroupdf.columns = dfName+['target','label']\n",
    "        \n",
    "        features_df = newsgroupdf[dfName]\n",
    "        features = features_df.as_matrix()\n",
    "        \n",
    "        target_df = newsgroupdf[['target']]\n",
    "        target = target_df.as_matrix()\n",
    "        \n",
    "        \n",
    "        #following code adapted from: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "        selector = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.mutual_info_classif, k)\n",
    "        selector.fit(features, target)\n",
    "        # Get idxs of columns to keep\n",
    "        idxs_selected = selector.get_support(indices=True)\n",
    "        # Create new dataframe with only desired columns, or overwrite existing\n",
    "        features_dataframe_new = features[idxs_selected]\n",
    "    \n",
    "    \n",
    "        final_set = {}\n",
    "        for index in idxs_selected:\n",
    "            final_set[dfName[index]] = newsgroupdf[dfName[index]]\n",
    "        final_set['target'] = newsgroupdf['target']\n",
    "        final_set['label'] = newsgroupdf['label']\n",
    "        \n",
    "\n",
    "\n",
    "        if size != None:\n",
    "            data_total = pd.DataFrame(final_set)\n",
    "            data_size = size/len(data_total)\n",
    "            data = data_total.sample(axis=0, frac=data_size, replace=False)\n",
    "        else:\n",
    "            data = pd.DataFrame(final_set)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ###########         end         ###########\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "data = dataPreprocessor(k=200, fs='mi', encode='tf', directorydf=directorydf, size=None)\n",
    "\n",
    "#data = dataPreprocessor(k=50, fs='mi', encode='tf', directorydf=directorydf, size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abortion</th>\n",
       "      <th>access</th>\n",
       "      <th>advance</th>\n",
       "      <th>agree</th>\n",
       "      <th>alt</th>\n",
       "      <th>among</th>\n",
       "      <th>anything</th>\n",
       "      <th>apple</th>\n",
       "      <th>aramis</th>\n",
       "      <th>argument</th>\n",
       "      <th>...</th>\n",
       "      <th>version</th>\n",
       "      <th>video</th>\n",
       "      <th>war</th>\n",
       "      <th>way</th>\n",
       "      <th>win</th>\n",
       "      <th>window</th>\n",
       "      <th>windows</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abortion  access  advance  agree  alt  among  anything  apple  aramis  \\\n",
       "0         0       0        0      0    8      0         1      0       0   \n",
       "1         0       0        0      1   10      0         1      0       0   \n",
       "2         0       0        0      0    1      0         0      0       0   \n",
       "3         0       0        0      0    4      0         0      0       0   \n",
       "4         0       0        0      0    2      0         0      0       0   \n",
       "\n",
       "   argument  ...    version  video  war  way  win  window  windows  would  \\\n",
       "0         0  ...          2      0    0    2    0       0        0      0   \n",
       "1         0  ...          0      0    0   13    0       0        0      7   \n",
       "2         3  ...          0      0    0    1    0       0        0      4   \n",
       "3         0  ...          0      0    0    0    0       0        0      0   \n",
       "4         0  ...          0      0    0    0    0       0        0      0   \n",
       "\n",
       "   year  years  \n",
       "0     0      0  \n",
       "1     0      2  \n",
       "2     0      0  \n",
       "3     0      0  \n",
       "4     0      0  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data.to_csv('top500-wordnetmod.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "name\n",
      "x\n",
      "austin\n"
     ]
    }
   ],
   "source": [
    "test = ['hello', 'my', 'name', 'x', 'austin']\n",
    "for word in test:\n",
    "    if nltk.corpus.wordnet.synsets(word):\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) For ten trials of randomized train/test splits, report the training accuracy and test accuracy of each trial as well as the average and 95% confidence intervals. What do you observe about their comparison and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSplitCI(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: 2D numpy array. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        num_run: int. How many times you want to run for random evaluation?\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        train_scores: list. Results of trials\n",
    "        test_scores: list. Results of trials\n",
    "        train_mean: scalar. Average accuracy\n",
    "        test_mean: scalar. Average accuracy\n",
    "        train_ci: scalar. Confidence Interval\n",
    "        test_ci: scalar. Confidence Interval\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "    features = data.drop(['label', 'target'], axis=1).as_matrix()\n",
    "\n",
    "    target = data[['target']].as_matrix()\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_mean = 0\n",
    "    test_mean = 0\n",
    "    \n",
    "    train_ci = 0\n",
    "    test_ci = 0\n",
    "    \n",
    "    if (clf == 'LR'):\n",
    "        clfFn = LogisticRegression(C=params[\"c\"])\n",
    "    elif (clf == 'NB'):\n",
    "        clfFn = GaussianNB()\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    for i in range(num_run):\n",
    "        # x_train,      x_test,        y_train,      y_test\n",
    "        features_train, features_test, target_train, target_test = train_test_split(features, target, stratify=target, test_size=0.3)\n",
    "        # train_features, test_features, target_train, target_test = train_test_split(features, target, test_size=0.3)\n",
    "    \n",
    "        # train the features and target datasets and fit to a model\n",
    "        clfModel = clfFn.fit(features_train, target_train)\n",
    "\n",
    "        # predict target with feature test set using trained model\n",
    "        target_pred = clfModel.predict(features_test)\n",
    "        \n",
    "        # predict target with feature train test set using trained model\n",
    "        feature_pred = clfModel.predict(features_train)\n",
    "        \n",
    "        train_scores.append(metrics.accuracy_score(target_train, feature_pred))\n",
    "        test_scores.append(metrics.accuracy_score(target_test, target_pred))\n",
    "        \n",
    "    train_mean = np.mean(train_scores)\n",
    "    test_mean = np.mean(test_scores)\n",
    "    \n",
    "    train_ci_full = st.t.interval(0.95, len(train_scores)-1, loc=train_mean, scale = st.sem(train_scores))\n",
    "    test_ci_full = st.t.interval(0.95, len(test_scores)-1, loc=test_mean, scale = st.sem(test_scores))\n",
    "\n",
    "    train_ci = train_mean - train_ci_full[0]\n",
    "    test_ci = test_mean - test_ci_full[0]\n",
    "\n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_scores,test_scores,train_mean,test_mean,train_ci,test_ci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train    \n",
      "Result of trails:[0.96384939629920696, 0.96527827391583909, 0.96406372794170181, 0.96434950346502823, 0.96506394227334424, 0.96327784525255411, 0.96242051868257483, 0.96377795241837538, 0.96334928913338569, 0.96349217689504896]     \n",
      "Average Accuracy: 0.9638922626277058     \n",
      "Confidence Interval: 0.0006100680927253466\n",
      "\n",
      "Test    \n",
      "Result of trails:[0.94999999999999996, 0.94350000000000001, 0.94599999999999995, 0.94933333333333336, 0.94466666666666665, 0.94933333333333336, 0.95266666666666666, 0.94899999999999995, 0.95116666666666672, 0.94783333333333331]     \n",
      "Average Accuracy: 0.9483500000000001     \n",
      "Confidence Interval: 0.0020585077754199377\n"
     ]
    }
   ],
   "source": [
    "train_scores,test_scores,train_mean,test_mean,train_ci,test_ci = randomSplitCI(data, 'LR', 10, c=1.0)\n",
    "print(\"Train\\\n",
    "    \\nResult of trails:{0} \\\n",
    "    \\nAverage Accuracy: {1} \\\n",
    "    \\nConfidence Interval: {2}\\n\".format(train_scores, train_mean, train_ci)\n",
    "     )\n",
    "print(\"Test\\\n",
    "    \\nResult of trails:{0} \\\n",
    "    \\nAverage Accuracy: {1} \\\n",
    "    \\nConfidence Interval: {2}\".format(test_scores, test_mean, test_ci)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LR\n",
    "Train    \n",
    "Result of trails:[0.96527827391583909, 0.96434950346502823, 0.96520683003500751, 0.96384939629920696, 0.96484961063084951, 0.96277773808673284, 0.96420661570336497, 0.96320640137172253, 0.96320640137172253, 0.96313495749089095]     \n",
    "Average Accuracy: 0.9640065728370365     \n",
    "Confidence Interval: 0.0006530796155977958\n",
    "\n",
    "Test    \n",
    "Result of trails:[0.94733333333333336, 0.94933333333333336, 0.94416666666666671, 0.94316666666666671, 0.94699999999999995, 0.94850000000000001, 0.94833333333333336, 0.94516666666666671, 0.94550000000000001, 0.94899999999999995]     \n",
    "Average Accuracy: 0.94675     \n",
    "Confidence Interval: 0.001532260922333717\n",
    "\n",
    "For NB\n",
    "Train    \n",
    "Result of trails:[0.59191255268986209, 0.60398656855040367, 0.57748088876187753, 0.56726441380295778, 0.56554976066299922, 0.58712581267414443, 0.58598271058083873, 0.58784025148246055, 0.5671929699221262, 0.55740515824819603]     \n",
    "Average Accuracy: 0.5791741087375866     \n",
    "Confidence Interval: 0.010422248985307192\n",
    "\n",
    "Test    \n",
    "Result of trails:[0.5665, 0.5718333333333333, 0.55000000000000004, 0.54149999999999998, 0.54216666666666669, 0.5658333333333333, 0.55233333333333334, 0.55649999999999999, 0.53766666666666663, 0.52449999999999997]     \n",
    "Average Accuracy: 0.5508833333333334     \n",
    "Confidence Interval: 0.010598400861842161\n",
    "\n",
    "Training accuracy and confidence interval is better than the test accuracy and confidence interval. This is expected given that the model is fit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) What do the average and 95% confidence intervals tell you? Are they more informative than a single trial? Yes or no, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A 95% confidence interval is a range of values that you can be 95% certain contains the true mean of the sample (level of certainty of the true mean). They are more informative than a single trial because a 95% confidence interval implies that if same data were to be sampled on numerous occasions and interval estimates are made on each occasion, the resulting intervals would bracket the true mean in approximately 95 % of the cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) For multiclass only: compute the confusion matrix by filling up Function randomSplitCM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSplitCM(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        cm: pandas.DataFrame. Confusion Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "    \n",
    "    \n",
    "    features = data.drop(['label', 'target'], axis=1).as_matrix()\n",
    "\n",
    "    target = data[['target']].as_matrix()\n",
    "\n",
    "    cms = {}\n",
    "        \n",
    "    labels = data['target'].unique()\n",
    "\n",
    "\n",
    "    if (clf == 'LR'):\n",
    "        clfFn = LogisticRegression(C=params[\"c\"])\n",
    "    elif (clf == 'NB'):\n",
    "        clfFn = GaussianNB()\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for i in range(num_run):\n",
    "        # x_train,      x_test,        y_train,      y_test\n",
    "        features_train, features_test, target_train, target_test = train_test_split(features, target, stratify=target, test_size=0.3)\n",
    "\n",
    "\n",
    "        # train the features and target datasets and fit to a model\n",
    "        clfModel = clfFn.fit(features_train, target_train)\n",
    "\n",
    "        # predict target with feature test set using trained model\n",
    "        target_pred = clfModel.predict(features_test)\n",
    "\n",
    "        # predict target with feature train test set using trained model\n",
    "        feature_pred = clfModel.predict(features_train)\n",
    "            \n",
    "        cms[i] = pd.DataFrame(metrics.confusion_matrix(target_test, target_pred), columns=labels, index=labels)\n",
    "\n",
    "\n",
    "\n",
    "    # Panel of all test set confusion matrices\n",
    "    pl = pd.Panel(cms)\n",
    "    cm = pl.sum(axis=0) #Sum the confusion matrices to get one view of how well the classifiers perform\n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/shagun/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cm = randomSplitCM(data, 'LR', 10, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) For multiclass only: show the confusion matrix (or a subset if too large to view at once). Are some classes more easily confused with others? Which ones and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2859</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2912</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>2900</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>2926</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>108</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2759</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2934</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2966</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2950</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2984</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2966</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2955</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2968</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2710</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2902</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>128</td>\n",
       "      <td>2377</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>189</td>\n",
       "      <td>2367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
       "0   2445     0     0     0     8     0     0     0     4     0     0     4   \n",
       "1      0  2859    24    23     4    47     3     0     1     0     0     0   \n",
       "2      0    27  2912    26     0    23     2     0     0     0     0     0   \n",
       "3      0    16    21  2900    27    20     6     0     0     0     0     0   \n",
       "4      0    10     0    49  2926     0     8     1     0     0     0     0   \n",
       "5      0    70   108    38     0  2759     3     0     1     0     0     1   \n",
       "6      0     9     2    17    11     0  2934     9    10     1     0     0   \n",
       "7      0     0     2     0     0     0    10  2966     4     2     0     0   \n",
       "8      0     0     0     0     8     0    14     8  2950     8     0     0   \n",
       "9      0     0     0     0     0     0     0     0     0  2984    16     0   \n",
       "10     0     0     0     0     0     0     0     0     0    19  2981     0   \n",
       "11     0     4     0     3     6     0     0     0     0     0     0  2966   \n",
       "12     0     1     0     4     5     0    11    13     0     0     0     0   \n",
       "13     1    10     3     0     1     1     0     0     1     0     0     2   \n",
       "14     0     4     0     0     0     0     0     0     0     0     0     0   \n",
       "15     8     0     0     0     0     1     0     0     1     0     0     0   \n",
       "16     0     5     0     3     3     0     0    12    13     1     0     6   \n",
       "17     0     0     0     0     0     0     0     0     0     0     0     0   \n",
       "18     0     0     3     0     1     0     0     0     0     1     0     0   \n",
       "19   350     0     0     0     1     0     0     0     0     0     0     0   \n",
       "\n",
       "      12    13    14    15    16    17    18    19  \n",
       "0      0     0     0     0     2     2     0   535  \n",
       "1      4    29     5     0     0     0     1     0  \n",
       "2      0     6     4     0     0     0     0     0  \n",
       "3      6     0     4     0     0     0     0     0  \n",
       "4      5     1     0     0     0     0     0     0  \n",
       "5      5     9     6     0     0     0     0     0  \n",
       "6      7     0     0     0     0     0     0     0  \n",
       "7     16     0     0     0     0     0     0     0  \n",
       "8      0     4     3     0     5     0     0     0  \n",
       "9      0     0     0     0     0     0     0     0  \n",
       "10     0     0     0     0     0     0     0     0  \n",
       "11     3    10     2     0     1     0     4     1  \n",
       "12  2955     9     2     0     0     0     0     0  \n",
       "13     4  2968     3     0     3     0     2     1  \n",
       "14     3     9  2976     0     1     0     5     2  \n",
       "15     0     0     0  2990     0     0     0     0  \n",
       "16     6     0     0     0  2710     2   176    63  \n",
       "17     0     0     0     0    11  2902    82     5  \n",
       "18     0     1     1     0   184   128  2377   304  \n",
       "19     0     4     1     0    86     2   189  2367  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 0 and Class 19 are easily confused with one another because they are similar topics - \"athesim\" vs \"religion\" - therefore the terms in those classes are likely to be similar.\n",
    "Classes with more unique classes such as class 11 (crypt) are less likely to be confused with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Performance vs. # Features Selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Write a function featureSizeAC which takes preprocessed data and classifier name as input, and outputs train and test accuracy for each of the feature size settings (k ∈ {10%, 20%, ..., 90%, 100%})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSizeAC(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        train_mean_fs: list.\n",
    "        train_ci_fs: list.\n",
    "        test_mean_fs: list.\n",
    "        test_ci_fs: list.\n",
    "        \n",
    "    \"\"\"\n",
    "    feature_precentage = np.linspace(0.1, 1, 10, endpoint=True)\n",
    "    ########### your code goes here ###########\n",
    "    \n",
    "\n",
    "    #create features dataframe from data by dropping label and target coloums\n",
    "    features_set = data.drop(['label', 'target'], axis=1)\n",
    "    \n",
    "    #confirm shape of features_set dataframe\n",
    "    #print (features_set.shape)    \n",
    "    \n",
    "    target = data[['target']].as_matrix()\n",
    "\n",
    "    train_scores_fs = []\n",
    "    test_scores_fs = []\n",
    "    temp_train_mean = 0\n",
    "    temp_test_mean = 0\n",
    "    train_mean_fs = []\n",
    "    test_mean_fs = []\n",
    "    temp_train_ci = 0\n",
    "    temp_test_ci = 0\n",
    "    train_ci_fs = []\n",
    "    test_ci_fs = []\n",
    "        \n",
    "\n",
    "    if (clf == 'LR'):\n",
    "        clfFn = LogisticRegression(C=params[\"c\"])\n",
    "    elif (clf == 'NB'):\n",
    "        clfFn = GaussianNB()\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    #iterate through feature size 10%, 20%, 30%... \n",
    "    for percentage in feature_precentage:\n",
    "        \n",
    "        #select a random sample of x% of features from features data frame, where x = percentage \n",
    "        #reference: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "        features = features_set.sample(axis=1, frac=percentage, replace=False).as_matrix() \n",
    "        \n",
    "        train_scores_fs = []\n",
    "        test_scores_fs =[]\n",
    "        \n",
    "        #run num_run trials for the selected feature size\n",
    "        for i in range(num_run):\n",
    "            # x_train,      x_test,        y_train,      y_test\n",
    "            features_train, features_test, target_train, target_test = train_test_split(features, target, stratify=target, test_size=0.3)\n",
    "            # train the features and target datasets and fit to a model\n",
    "            clfModel = clfFn.fit(features_train, target_train)\n",
    "\n",
    "            # predict target with feature test set using trained model\n",
    "            target_pred = clfModel.predict(features_test)\n",
    "\n",
    "            # predict target with feature train test set using trained model\n",
    "            feature_pred = clfModel.predict(features_train)\n",
    "        \n",
    "            #build an array conatining test and training scores for each run of the selected sample size\n",
    "            train_scores_fs.append(metrics.accuracy_score(target_train, feature_pred))\n",
    "            test_scores_fs.append(metrics.accuracy_score(target_test, target_pred))\n",
    "        \n",
    "        \n",
    "        temp_train_mean = np.mean(train_scores_fs)\n",
    "        temp_test_mean = np.mean(test_scores_fs)\n",
    "        \n",
    "        train_mean_fs.append(temp_train_mean)\n",
    "        test_mean_fs.append(temp_test_mean)\n",
    "        \n",
    "        temp_train_ci = st.t.interval(0.95, len(train_scores_fs)-1, loc=temp_train_mean, scale = st.sem(train_scores_fs))\n",
    "        temp_test_ci = st.t.interval(0.95, len(test_scores_fs)-1, loc=temp_test_mean, scale = st.sem(test_scores_fs))\n",
    "        \n",
    "        train_ci_fs.append(temp_train_mean - temp_train_ci[0])\n",
    "        test_ci_fs.append(temp_test_mean - temp_test_ci[0])    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs = featureSizeAC(data, 'LR', 10, c=1.0, r=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) use your code to produce a plot with the train accuracy (blue) and test accuracy (red) as a function of the number of features selected (10%, 20%, ..., 90%, 100%). Explain any trends you see (average over multiple trials if trends are not clear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSizePlot(feature_size, train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(feature_size, test_mean_fs, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_fs)\n",
    "    train_curve=plt.errorbar(feature_size, train_mean_fs,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_fs)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Feature Percentage')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Feature Size\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129821080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featureSizePlot(np.linspace(0.1, 1, 10, endpoint=True),train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy increaes are more features are used. This makes sense given that initial k features were picked for maximum accuracy - fewer features = lesser distrciminating power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Write a function hyperparameterAC which takes preprocessed data and a classifier name as input, and outputs train and test accuracy for each of the hyperparameter settings (C ∈ {10−4, 10−3, ..., 100,... 103, 104})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperParameterAC(data, clf, num_run):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "    \n",
    "    OUTPUT\n",
    "        test_mean_hp: list.  mean accuracy list of test\n",
    "        test_ci_hp: list. confidence interval list of test\n",
    "        train_mean_hp: list. mean accuracy list of train\n",
    "        train_ci_hp: list. confidence interval list of train\n",
    "        \n",
    "    NOTE \n",
    "        randomSplitCI could be the sub-route of this function\n",
    "    \"\"\"\n",
    "    params = np.logspace(-4, 4, num=9)\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "    #create features array from data, also drop label and target coloums\n",
    "    features = data.drop(['label', 'target'], axis=1).as_matrix()\n",
    "\n",
    "    target = data[['target']].as_matrix()\n",
    "\n",
    "    train_scores_hp = []\n",
    "    test_scores_hp = []\n",
    "    temp_train_mean = 0\n",
    "    temp_test_mean = 0\n",
    "    train_mean_hp = []\n",
    "    test_mean_hp = []\n",
    "    temp_train_ci = 0\n",
    "    temp_test_ci = 0\n",
    "    train_ci_hp = []\n",
    "    test_ci_hp = []\n",
    "        \n",
    "\n",
    "    if (clf != 'LR'):\n",
    "        return\n",
    "    \n",
    "    #iterate through each hyperparameter setting \n",
    "    for c in params:\n",
    "        clfFn = LogisticRegression(C=c)\n",
    "        \n",
    "        train_scores_hp = []\n",
    "        test_scores_hp =[]\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            # x_train,      x_test,        y_train,      y_test\n",
    "            features_train, features_test, target_train, target_test = train_test_split(features, target, stratify=target, test_size=0.3)\n",
    "            # train the features and target datasets and fit to a model\n",
    "            clfModel = clfFn.fit(features_train, target_train)\n",
    "\n",
    "            # predict target with feature test set using trained model\n",
    "            target_pred = clfModel.predict(features_test)\n",
    "\n",
    "            # predict target with feature train test set using trained model\n",
    "            feature_pred = clfModel.predict(features_train)\n",
    "        \n",
    "            train_scores_hp.append(metrics.accuracy_score(target_train, feature_pred))\n",
    "            test_scores_hp.append(metrics.accuracy_score(target_test, target_pred))\n",
    "        \n",
    "        temp_train_mean = np.mean(train_scores_hp)\n",
    "        temp_test_mean = np.mean(test_scores_hp)\n",
    "        \n",
    "        train_mean_hp.append(temp_train_mean)\n",
    "        test_mean_hp.append(temp_test_mean)\n",
    "                \n",
    "        temp_train_ci = st.t.interval(0.95, len(train_scores_hp)-1, loc=temp_train_mean, scale = st.sem(train_scores_hp))\n",
    "        temp_test_ci = st.t.interval(0.95, len(test_scores_hp)-1, loc=temp_test_mean, scale = st.sem(test_scores_hp))\n",
    "        \n",
    "        train_ci_hp.append(temp_train_mean - temp_train_ci[0])\n",
    "        test_ci_hp.append(temp_test_mean - temp_test_ci[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp = hyperParameterAC(data, 'LR', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Now that you’ve implemented hyperparameterAC, use the code following it to plot train accuracy (blue) and test accuracy (red) as a function of the hyperparameter setting (10−4, 10−3, ..., 100,... 103, 104). Use a logarithmic x-axis. Explain any trends you see (average over multiple trials if trends are not clear). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperParameterPlot(params, train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(params, test_mean_hp, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_hp)\n",
    "    train_curve=plt.errorbar(params, train_mean_hp,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_hp)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Parameter')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Parameters\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1296ef128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperParameterPlot(np.logspace(-4, 4, num=9),train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is inverse regularization strength - smaller values specify stronger regularization.\n",
    "If regularlization is too strong (small hyperparameter value) accuracy decreases because model is not able to fit data. If regularization is too weak, the model will overfit the data - as a result, training data will perform well but test data will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Write a function dataSizeAC which takes preprocessed data and classifier name as input, and outputs train and test accuracy for each of the training data size settings (size ∈ {10%, 20%, ..., 90%, 100%})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataSizeAC(data, clf, num_run):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        test_mean_ds: list.  mean accuracy list of test\n",
    "        test_ci_ds: list. confidence interval list of test\n",
    "        train_mean_ds: list. mean accuracy list of train\n",
    "        train_ci_ds: list. confidence interval list of train\n",
    "        \n",
    "        \n",
    "        \n",
    "    NOTE \n",
    "        randomSplitCI could be the sub-route of this function\n",
    "    \"\"\"\n",
    "    data_precentage = np.linspace(0.1, 1, 10, endpoint=True)\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "    #create features array from data, also drop label and target coloums\n",
    "    features = data.drop(['label', 'target'], axis=1).as_matrix()\n",
    "\n",
    "    target = data[['target']].as_matrix()\n",
    "    \n",
    "    if (clf == 'LR'):\n",
    "        clfFn = LogisticRegression()\n",
    "    elif (clf == 'NB'):\n",
    "        clfFn = GaussianNB()\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    temp_train_scores = []\n",
    "    temp_test_scores = []\n",
    "    train_scores_ds = np.empty((0, 10))\n",
    "    test_scores_ds = np.empty((0, 10))\n",
    "    temp_train_mean = []\n",
    "    temp_test_mean = []\n",
    "    train_mean_ds = []\n",
    "    test_mean_ds = []\n",
    "    temp_train_ci = 0\n",
    "    temp_test_ci = 0\n",
    "    train_ci_ds = []\n",
    "    test_ci_ds = []\n",
    "    \n",
    "    for i in range(num_run):\n",
    "        \n",
    "        # x_train,            x_test,        y_train,            y_test\n",
    "        features_train_trial, features_test, target_train_trial, target_test = train_test_split(features, target, stratify=target, test_size=0.3)\n",
    "        \n",
    "        #print ('trial', i)\n",
    "        #print ('shape of initial split - training', features_train_trial.shape)\n",
    "        \n",
    "        temp_train_scores = []\n",
    "        temp_test_scores = []\n",
    "        \n",
    "        \n",
    "        for percentage in data_precentage:\n",
    "            if percentage < 1.0:\n",
    "                training_percent =  percentage #.7 *\n",
    "                # x_train,      x_test, y_train,      y_test\n",
    "                features_train, _,      target_train, _ = train_test_split(features_train_trial, target_train_trial, test_size=0.0, train_size = training_percent)\n",
    "            else:\n",
    "                # x_train,      x_test, y_train,      y_test\n",
    "                features_train, _,      target_train, _ = train_test_split(features_train_trial, target_train_trial, test_size=0.0)\n",
    "           \n",
    "            # train the features and target datasets and fit to a model\n",
    "            clfModel = clfFn.fit(features_train, target_train)\n",
    "\n",
    "            # predict target with feature test set using trained model\n",
    "            target_pred = clfModel.predict(features_test)\n",
    "\n",
    "            # predict target with feature train test set using trained model\n",
    "            feature_pred = clfModel.predict(features_train)\n",
    "            \n",
    "            temp_train_scores.append(metrics.accuracy_score(target_train, feature_pred))\n",
    "            temp_test_scores.append(metrics.accuracy_score(target_test, target_pred))\n",
    "        \n",
    "        train_scores_ds = np.append(train_scores_ds, [temp_train_scores], axis=0)\n",
    "        test_scores_ds = np.append(test_scores_ds, [temp_test_scores], axis=0)\n",
    "        #print('train_scores_ds           :, train_scores_ds)\n",
    "    \n",
    "    train_mean_ds = np.mean(train_scores_ds, axis=0).tolist()\n",
    "    test_mean_ds = np.mean(test_scores_ds, axis=0).tolist()\n",
    "    #print ('all mean scores', train_mean_ds)\n",
    "\n",
    "    #ci_train_score = np.transpose(train_scores_ds)\n",
    "    #print (ci_train_score)\n",
    "    #print ('this is row 1', ci_train_score[0,:])\n",
    "    \n",
    "    #ci_train_score = []\n",
    "    #ci_test_score = []\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for score in range(10):\n",
    "        ci_train_score = np.transpose(train_scores_ds[:,score])\n",
    "        temp_train_mean = train_mean_ds[score]\n",
    "        temp_train_ci = st.t.interval(0.95, len(ci_train_score)-1, loc=temp_train_mean, scale = st.sem(ci_train_score))\n",
    "        train_ci_ds.append(temp_train_mean - temp_train_ci[0])\n",
    "        \n",
    "    for score in range(10):\n",
    "        ci_test_score = np.transpose(test_scores_ds[:,score])\n",
    "        temp_test_mean = test_mean_ds[score]\n",
    "        temp_test_ci = st.t.interval(0.95, len(ci_test_score)-1, loc=temp_test_mean, scale = st.sem(ci_test_score))\n",
    "        test_ci_ds.append(temp_test_mean - temp_test_ci[0])\n",
    "        \n",
    "\n",
    "    ###########         end         ###########\n",
    "    return train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shagun/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds = dataSizeAC(data, 'LR', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Now that you’ve implemented dataSizeAC, use the code following it to plot train accuracy (blue) and test accuracy (red) as a function of the amount of train data (10%, 20%, ..., 90%, 100%) while holding the amount of test data fixed. Explain any trends you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataSizePlot(datasize, train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(datasize, test_mean_ds, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_ds)\n",
    "    train_curve=plt.errorbar(datasize, train_mean_ds,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_ds)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Data Percentage')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Data Percentage\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVNX5+PHPzGxvsAvL0pvig9IUUIpix1hiS6ImJCaa\nEGMsMSaaYkyxpH1/GuP3qxCNLaZoNIgtdqIRC4igsBQfQKSpwC67bGHZMjvz++PcnZ1dtwyws7Pl\neb9e+5qZW585DPe555x7z/WFw2GMMcYYAH+iAzDGGNN1WFIwxhgTYUnBGGNMhCUFY4wxEZYUjDHG\nRFhSMMYYE5GU6ABM9yEiycAWYJWqnp7oeBJFRMLAaqAe8HmT/6qqt8Ww7lwgRVXn7cf+LgHuBD4C\nwt4+9wLXqerb+xd9xxGRUcBtqvrFRMVgOp7VFMz+OB9YBUwRkcMTHUyCnaSqR6rqJOAU4Esi8sMY\n1jsOyDiA/S329neUqh4J/A54QkQSeWI3ApAE7t/EgdUUzP64AngU2Ah8H/gOgIh8E/gh7sy5GPiG\nqm5raTpwCHCXqo731j2x4bOI/AqYAQzCJZ8fAvcABcBAXC3lQlXdJSKHefMGACHgVmC7F98IVQ2J\nSAawGRivqru8/fm97Zyvqu960x4F/gu8CtwPpOHOxu+L5YxeVYtF5AfAAhH5gxfTZ+IGjgXOAWaL\nyD7gX619v/b2CSzy1ukrIuXA74ETgADwHvA9VS0Xkc3AUmAicAOwpnm5qeo/RWQIcBcwHEgGHlXV\n34jISG9fzwHTgDzgZ17s9wFDRORFVf2ciNwAnOeVXyauJrPQ+3f4EzAd2AOs9crtktb2G8P3N3Fi\nNQUTExE5Avef+jHgL8DFItJPRCbhDkinq+pE4GngZ61Nj2FXI4DJqvo14MvA26o6AxgNVAEXe8s9\nCjyuquOAM4HfAIXAbqChaevLwKLog6yqhoAHgEu875ULzAb+AVwPPKOqU7xtHu8lkVisxB2k+7cW\nt6ou9MrhDlW9u53v1yoR8QGXAatVtRj4CRAEpng1l09wNYkGq1X1cG//nyk3EckB/go84H33Y4BT\nReRCb/3RwIuqegzwY+B/VLUemAt86CWEEcCpwAnev/fPgJu99X+OOwEd6y1zVFRsbe3XJIDVFEys\nvgv8W1VLgBIR+QhXU6jGHTC2AajqHwG8M+eWpp/Yzn6WqGrQW+dOEZnlbWsMMB5YKiJ5wCTcmSre\nPg7xtn838G3cme13cAf65h4Alnnb/QouEZSJyELgYRE5BngFd7YdirF8GsaL2dda3M1XiHU5zywR\ned/bTyrwAdDQlv95oC+uBgKQAkTXNhYDtFZuIpKJq2Xkicgt3jpZwJHAO0AdrjwBVuBqC82/yxYR\n+QbwVRE5FHcCkeXNPhP4gVeW5SLyF2BiO/t9rJVyMHFmScG0y/vP+3Wg2muOAMgBrgT+h8YDIiKS\njjvbD7YyvaGjtEFKs91VRq3ze9zZ4wO4pp1kb92gt0j09gXYCvwdd/Z7EpClqq83/z7eAWwF7mB6\nKa4pDFV9VkTG4GoOpwC/FJGZqvphmwXkHA18pKqVbcTdRKzLeRar6udbmRcArlHV573tZuGacBo0\nlGlr5bbD2+9MVa3ypvfHJfz+QG1Ucmz+79ewncnAU8AdwEu45rj5UfuNXqc+Ku7W9msSxJqPTCy+\niusTGKyqI1V1JK5JIQt3hnqqiAzylv0OLlG82sr0ImC4iAzwmkHOa2O/nwP+qKp/xZ35zgYCqloO\nLMf1USAiw4A3gT7eweVvuAPtn9rY9p9xTSEZqvqmt51/ABep6qO4/pNyYFh7hSMig3FNZQ1XH7UY\ntzcviDv4t7fc/ngRuEpEUrzmrj8Dv22+UBvllg4sAX7gTe/rTT+3nf1Gf5fjgXdV9Q+4hHBe1Hf5\nN3CpiPi9/oU5QNiL50D2a+LIkoKJxXeBP3jtyACo6h7gf3Fn29cDL4jISlx7/uWqWtjK9LW4js53\ncQeET9vY783AbSKyHHgCeAM41Js3B7jQ2/YzwFxV3eHNexDXkfpwG9t+GhiJ61hucAuu+WMlrhln\nIe4A15JXReR9L7angYejOqXbivt54Hsi8tN2ltsft+A61N/DdeL6cJ30LWmt3OYA00Wk0Pvuj6jq\n39vZ7xqgXkTeAR4B+ovIWlziqcQ1C2XjElQ1rs/nFVwCrIqKZ3/3a+LIZ0Nnm57Eq338GHcF0ncT\nHY8BEfkyUK6qz3k1mQXAS6o6v51VTQJYn4LpaTbhmqjOSXQgJmI1cI+I/AbXh/QqXme36XriWlMQ\nkWnA71X1xGbTzwZ+gWuTfEBV/+ydQczDXR1Rg6vWboxbcMYYYz4jbn0KIvIj3NlAWrPpybgrFE7D\nXY52mYgU4N304l2z/RPg9njFZowxpmXx7Gj+EPhCC9MPBzaqaqmq1uI6147H3f7/AoCqLgGmxjE2\nY4wxLYhbn4KqLvBukW8uByiL+lwB9Glher2IJDXcyNSaYLA+nJR0IFfxGWNMr9biPTGJ6GguB7Kj\nPmfjxkNpPt3fXkIAKC2tam+RLi8/P5uioopEh9ElWFk0ZeXRlJVHo4Mti/z87BanJyIprAPGeLfc\nV+Kajm7D3Sl5NvCYiEzHXdNsjDGmE3XazWsiMkdELlPVOtwdjC8Cb+OuPvoYd6NQtYi8heuIvraz\nYjPGGON0+5vXiooquvcXwKrE0awsmrLyaMrKo1EHNB+12Kdgw1wYY4yJsKRgjDEmwpKCMcaYCEsK\nxhgTo7Ibr6HsxmsSHUZcWVIwxhgT0WtHSb3s5scBuPcXFyQ4EmNMb/R//3cHqusoKdlNdXU1gwcP\noW/fXG699fcxrb99+3befXcVxx47q0Pj6rVJwRhjEunqq92tWM899wxbtmzmu9+9er/Wf/vtt1m/\nfpMlBWOM6WhVT/yDuhVL210utKcEIKZ+heTJ08j4wpz9jmXevDspLFxFKBRizpyLOeGEk3n88Ud5\n6aXn8fv9jB8/gcsvv5r77ruPffuqGT9+IjNnHrff+2mNJQVjjOki3njjdYqKipg//35qaqq57LJL\nmDr1GJ577ml++tNfMGaMsHDhv/D7/cydO5f16zd1aEKAXpwUamqD1IfC1NYFSUnutcVgjAF3Rh/D\nWX1DDaHPrXfGJY5Nmzaybt1arrrqMgDq6+vZuXMHN954M4888ld27PiUCRMmEc+RKHrt1UfVtUH2\n7qvle797kiWrtsS1kI0xJhYjRoxk6tRjuOuue7nzzvmcdNKpDBo0hGeeWciPfvQz7rrrXtauXc3a\ntavx+XxxOW712lPk7MxUqvbVUrxnL79/8FUmjhnEt84/huGDchMdmjGmlzr++JN4770VXHHFXPbt\nq+LEE08hPT2dkSNHceWVc0lPz2DAgALGjj2CkpK+/OlP9zBmjHDyyad2WAy9dkC8hktSf3n5aTzw\n5DusWPcxfr+PM44dy5dPP5KsjNQOjbMtNshXIyuLpqw8mkp0ecS7+Wh/xGtAvF5bU2gwZEAffn7Z\nbN5ds40HnlrGvxev4/UVm5hzxlHMnnEYAX+vbWEzxjTTFZJBvNkRzzN13DDu/NG5fP3sqQSDIe75\n1xKuu/0ZVm/ckejQjDGm0/Ta5qO2lJZX8bd/r+A/72wEYOakkXzjnKkMyMvq6F0Bia8SdyVWFk1Z\neTRl5dHImo86UW5OBld/5Tg+N1O4f+FS3lq5mXfXbuP8k8dz/skTSE2xYjPG9ExxO7qJiB+YB0wC\naoC5qroxav7FwPVAGfCQqt4vIqnAg8BooBy4UlU3xCvG9hw2Ip/ffu8s/rv8Q/76zHL++eJK/vPO\nRr5xztHMnDQCn6/FRGuM6aF6w5hp8exTOA9IU9UZwE+A2xtmiEh/4BbgROAE4KsiMhL4NlCpqtOB\nq4G74hhfTPx+HycdfSh33/AFzj95PKXl+7jtL6/x87tfYPMnJYkOzxhjOlQ820GOA14AUNUlIjI1\nat5oYKWqlgCIyDJgOnAE8Ly3jorI4XGMb7+kpyXz9bOnMnv6YTz49DKWrd7GD297htNmHsZXTj+K\nnKy0RIdojOlGDnSU1A0blDfeeJ0f/egHcYkrbh3NInIfsEBVn/c+bwVGq2pQRHKBZcCxQAXwOjAf\nCADTgLne65tAiqrWt7afYLA+nJQUiMt3aMvSVVv4378tZssnpWRnpvKtL07jvFMmkBSwC7qM6am+\n9P2HAPjXHy/psG0+8cQTbNq0ieuuu67DthmjTu9oLgeyoz77VTUIoKqlInItsADYDawAioF/A4cD\ni3EJYXlbCQGgtLQqDqG3b/SgPG77wdk8/8Y6Hn3hff748OsseGkVc88/homHDd6vbdkVFY2sLJqy\n8mgqXuXx0NPLeOv9ze0ut7vMHW/Ov/qBdpedeeRILjnn6HaXq6iopqqqlqKiClaseJf58/+P5ORk\nzjnnfFJTU3niiccJBoP4fD5+85vb2LRpI089tYB58+7ilFNOZcKESWzduoW8vDxuvfV/CARiO0nO\nz89ucXo8T2vfBM4EEJHpQGHDDBFJAiYDs4ALgbHe8kcDi1T1OOBxYFMc4ztoSQE/Z58wjrtv+AKz\npx/G9p17+OX8l/jdA/9h5+7YfriX3fx45OzDGGNqa2uZN+8+Tj/9LLZt28r/+393Mn/+/YwcOYp3\n3nm7ybKffPIxc+dezj33PMiePaWsW7f2oPcfz5rCQmC2iLyFq6ZcKiJzgCxVvVdEwNUQqoHbVbXY\nm3aLiPwM2AN8K47xdZi+2elccdFMPjdTuG/hUpYWbmXFuu2ce9J4vnjKBNJSkxMdojGmDZecc3RM\nZ/WdcfXR8OEjIu9zc/O49dZfkpGRwZYtmxk/fmKTZfv06UtBwUAABgwooLa25qD3H7ekoKoh4PJm\nkz+Imn8TcFOzdYqBjhvZqZMdMqwfv7n6DBav+IiHn3mXf728ilff2cjXz57KrMmj7BJWY0y7/H53\nnKisrOT+++9hwYJnAbj22is/MypqPI4pdhdWB/P5fBw/ZTTHjB/GE4sKefLV1dzxt9d5/s0PmHv+\nNA4Z1i/RIRpjuoHMzEwmTJjE5ZdfSiCQRHZ2NsXFRQwatH99lvvLhrmIs527K3jo6XdZsmoLPh+c\nMm0MXz1zMn2z0wFXHQ0E/Mz/2RcTHGnXYB2rTVl5NJXo8uhKN6/ZMBfdVEG/bH586UmsWv8J9y18\nh1eWbOCt9zdz0eeO5MxZXeY2DGNMDLpCMog3u6i+k0w8bDB3XHcO3/7CNPx+Hw8+tYzv/89T1NYF\nEx2aMcZEWFLoRIGAnzNnHc7dP/0Cpx8rfFpUTlllDSVlVbyzeiv1oVCiQzTG9HLWp5BAH31cwo//\n+Cx1QZcMCvKyOOO4sZwybUynPvmtK0l0m3FXY+XRlJVHI+tT6IFGDcmjb3Y6oXCYKYcP5bV3P+Sh\np9/lkRfe54Qpozlr1uH2zGhjTKeypNAFJCcF+O6FM/naWVN4ZekGnn9jHS+9vZ6X3l7PhDGDOGvW\n4UwdN9QeDWqMiTtLCl1IdmYq5588nnNOPIJ312zj34s/oHDDpxRu+JQBXtPSqb24ackYE3+WFLqg\ngN/PtAkjmDZhBFs+LeW5xet47d0P+cvT7/Ko17R05qzDGWFNS8aYDmZJoYsbMSg30rS06J0NPP/G\nB82alsYyddwwa1oyxnQISwrdRHZmKuedNJ6zTziCd9ds57nF61gV1bR0+rGuaSk788CalrrSnZrG\nmMSxpNDNuKal4UybMDzStPTf5Zt4+Jl3efSF9zhhyiGcNetwRgy2piVjzP6zpJBg9/7iggO+3rih\naeniz09h0TsbeX7xOl5esp6Xl6xn/KEDOWvW4Rw93pqWjDGxs6TQA2RlpHLuieP4/PGHs3yta1pa\nuf5TVm/cQX5uJmccO5ZTpx92wE1Lxpjew5JCDxLw+zlm/HCOGT+cbTv28Nwb63h12Yc8/OxyHn3x\nfY73bogbOTgv0aEaY7ooSwo91LCBffnOl2bwtbMm88rSjTz/xge8smQDryzZwLhDCjjr+CM4Ztww\nAgFrWjLGNIpbUhARPzAPmATUAHNVdWPU/IuB64Ey4CFVvV9EkoG/ACOBeuDbqvpB822b2GWmNzYt\nrVj7Mf9evI6V6z9hzYc7yc/NdFctTR+T6DCNMV1EPGsK5wFpqjpDRKYDtwPnAohIf+AWYDLuWcyv\niMgiXAJJUtWZIjIb+DVgT5/pAAG/n6PHD+Po8cMiTUuvLfuQvz67nH+++D5+n4/UlCTqQyHrmDam\nF4vn//7jgBcAVHUJMDVq3mhgpaqWeM9yXgZMB9YDSV4tIweoi2N8vVZD09J9v7qAS889mtycDKpr\ng5RVVvOtXz7G3f98k+Vrt1MXrE90qMaYTha3obNF5D5ggao+733eCoxW1aCI5OISwbFABfA6MB94\nCXgKyAL6A59X1bfa2k8wWB9OSgrE5Tv0FvWhEOdd9QDVNUHSUpMoLd8HQEZaMjOOHMnxU0czY9JI\nMtJTEhypMaYDdfrQ2eVAdtRnv6oGAVS1VESuBRYAu4EVQDFwLfCiqv5URIYB/xGRCapa3dpOSkur\n4vYFOktXGCM+KeAnKyOF+Td+Ed1cxNLCrSxdtYVFSzawaMkGkgJ+Jh02mGkTh3P0uGGRZ0x3tK5Q\nFl2JlUdTVh6NOuB5Ci1Oj2dSeBM4G3jM61MobJghIkm4/oRZQArwMnADMJHGJqMSIBmwakAnCvj9\nHDG6gCNGF3DJOVPZ/EkpSwu3sLRwK8vXbWf5uu34fT7GjhrAtAnDmT5xOAPyWv5xGWO6n3gmhYXA\nbBF5C1dNuVRE5gBZqnqviICrIVQDt6tqsYjcATwgIotxyeIGVd0bxxhNG3w+H6OG5DFqSB5fPv0o\nPi0u553CrSwp3Mq6j3aydtNOHnxqGaOG5DF94gimTRjO8IF98flarJUaY7oBexxnF9AVqsT7OyBe\naXkVy1ZvY0nhVgo3fEqw3j1SdFD/bDfs98ThHDY8H79//xJEVyiLrsTKoykrj0b2OE7TpeTmZHDa\nTOG0mcLefbWsWLedJau2smLddp58dTVPvrqa3Jx0jhk/nOkThjPu0IEkt3NBwGU3P04g4Gf+z+wq\nZGMSxZKCOWiZ6SnMmjyaWZNHU1sXZOX6T1lauJVlq7fy4lvKi28pGWnJTB03jOkThnPU2CGkpSYn\nOmxjTAssKZgOlZKcxNHjhnH0uGHU189g3Ue7Ih3Vry/fxOvLN5GSHGCSDGb6hOFMHTeMnMy0RIdt\njPFYUjBAfB6uEwj4GX/oQMYfOpBvnncMm7aXsLRwC0sKt7Js9TaWrd6G3+/jiNEFTJ8wwt1NbWMx\nGZNQlhRMp/D5fBwyrB+HDOvHnDMn80lROUtXuQSxeuMOVm/cAUBSkp+/Pruc8YcOZOyoAaRbM5Mx\nncquPuoCevsVFSVlVbyzeisPPPkOdcFQZLrf7+PQYf0Yd8hAxh06kMNHDSAjrXfdVd3bfxvNWXk0\nsquPTI+V1yeD048dyxOLCvH5fVz+pRms+dDVHjZuLWb9lmIW/mc1fr+P0UP7Mf6QgYw7pIDDRxeQ\naUNvGNOhLCmYLsXv83HU2CEcNXYIANU1dXywuYjVG3ew5kOXJDZuLebJV1fj9/kYPTQvUpM4YvQA\nMtPt6XLGHAxLCqZLS0tN5kgZzJEyGHBJQjcXRWoSG7YWs3Hbbp56bQ1+n4+RQ/IYf6irSRwxuoCs\nDEsSxuwPSwqmW0lLTWaSDGaSlyRqaoPoliLWbNzB6g93sH5zEZu27+bp19bg88GowdE1iQJ7TrUx\n7bCkYLq11JQkJo4ZxMQxgwCXJDZsLYpc0bR+SxGbPi7hmdfX4vPBiEG5Xk1iIEccUmD3SBjTjF19\n1AXYFRWNOrosauuCrN9SHGluWr+liNq6xocHRZLEoQMZN7qAnKyulSTst9GUlUcju/rImAOQkpwU\nuYHuos9BXbCeDVuKWf3hDtZs3MEHm3ex5dNS/r14HQDDB/VlV0klyUkBfn3VGQzKzyHJbqgzvYgl\nBdOrJCcFOOKQAo44pABOm+SSxFZXk3BJooia2iDVNUG+9/snCfh9DM7vw9CBfRg+sC/DCvoydGBf\nBufntDvAnzHdkSUF06slJwUiDxW6YLZLEpfd/Dh1wRAzJo5g6449bN+5h2079/D2yi2R9fx+H4P6\n5zBsYF/3V9CH4QNzGTzAkoXp3iwpGBMlOSnAgzd/ucm0cDjM7rIqtu9wyWHrjj1s31HG1h2lfLyr\njCWrmiaLgf2yo5KFex0yIIeUZPvvZro++5Ua0w6fz0f/vpn075vJkd5NdeCSRWn5Prbt2NNYo/AS\nx9LCrSwt3BpZ1u/zUdA/m2EFfZokjCED+pCa0vJ/Q3u+hEmEuCUFEfED84BJQA0wV1U3Rs2/GLge\nKAMeUtX7ReQS4BJvkTTgSGCgqu6JV5zGHCifz0denwzy+mRE7psAlyz2VLhksW1HGdu8ZLF1xx7e\nWb2Nd1Zvi9oGFPTLjtQo3Gsfhhb0TcRXMiauNYXzgDRVnSEi04HbgXMBRKQ/cAswGdgDvCIii1T1\nIeAhb5m7gQcsIZjuxufzkZuTQW5OBhMPa5osyiqrI7UJlzTc37I121i2pmmy8Pl8JAX83P7wf8nO\nTCU7I5WsjFT33vucnZlKTmYqGWkpcXs29v4+qtV0b/FMCscBLwCo6hIRmRo1bzSwUlVLAERkGTAd\n2Ox9ngqMU9Ur29tJbm4GST2gYy8/PzvRIXQZPbksBgzIYczoAZ+ZXlq+j80fl/DR9t1s/qSEzR+X\n8v66j6mtq+eN9z5qd7sBv88liKw0+mSle69p5GSnkZOZSk5WOn2yvWlZja+tNV012bZ3SW5X+Xfp\nKnF0BfEoi3gmhRxc01CDehFJUtUgsAEYJyIFQAVwCrA+atkbgJti2UlpaVUHhZs4dkNOo95cFkP7\n5zC0fw6zjhwFuDN0v9/Hr686g4qqGiqraqjY6/1VRb1GTd9TUc32nWWEQrHd05mSHIiqdaRFah+R\nGklGKlX7avH7fWzaXExWRvxqJLHozb+P5jrg5rUWp8czKZQD0Xv1ewkBVS0VkWuBBcBuYAVQDCAi\nfQFR1VfjGJsx3YLP56Nf30z69c2MeZ1wOExVdV1U8qiOJI7KqmZJxXu/q6SSzZ+Utrndr9/4CBlp\nyQzIy6KgXzYD8rLc+7xsBvTLoiAvy5693QPEMym8CZwNPOb1KRQ2zBCRJFx/wiwgBXgZVzsAOB5Y\nFMe4jOkW7v3FBQd0Nujz+chMTyEzPYWBxN68EKwPfbY2UlXDQ0+9QygUZvyhg9hZUsGO4opWE0hO\nVhoFXrJoSB4FeVkM6JdNfm6m3cPRDcQzKSwEZovIW4APuFRE5gBZqnqviICrIVQDt6tqsbeeAJvi\nGJcxpgVJAT99s9Ppm53eZPpjL74PwA1zTwFcTaRibw07SyrZtbuCnSWV7Nxdwa6SSnaWVPLRxyVs\n2Fr8me37fJCXk9GkplHQL4sBedkU9Msir08GAb8NKZJocUsKqhoCLm82+YOo+TfRQr+Bqv6/eMVk\njDl4Pp+PHK+jeszw/p+ZHwqFKSmvYldJJbt2V7KzxEsYXuLQzUWs+2jXZ9YL+H30z83yahauWcol\nDff3ozueJSkpYPdtxJmNktoFWOdZIyuLpnpieQTrQxSVuoSxq7SSnbsr2VVSEXndU1Hd6roB75Gs\nkSuoMhuurmq8qio7M5U+WWmkpyYntFM83myUVGNMj5AU8DOofw6D+ue0OL+mNthYsyhtqG1U8u6a\nbYTCYT76uIRgfSim/TRcehtJHJlpn52WlUafzDSyMlLx+9tPIj39vg1LCsaYLiU1JSkyFEi0hmE/\n5t3wBaqq6yjfW015ZTXllTWU7a2morKashamfVpczkcfB9vdr9/nIysjpUmiiE4eDTWTYLAev99P\nOBzukTURaz7qAnpiE8GBsrJoysqjqQMtj9q6oEsUldVU7G1MHmWV7pLdskovmex10yqratrdZkpy\ngPxcNyZW/9ws+vfN8F4zI9NjuTnwQFnzkTHGHKCU5CT65ybRPze2+z3q60NUVNVEEkV5ZY2XMKp5\n8tXVhEJhhg7oQ/GevXy8q7zV7WRnpnpJwyWJ/Kj3/XMzycvJiNwx3lVYUjDGmGYCrVyeC7Bo6QYA\nbr/uHMD1gRTv2Utx6d4mr0Xe6ydF5Xz0cUmL+/H7fOT2SSe/bxb9czPo3zeraRLJzSQ7M7VTm6ms\n+agLsCaCRlYWTVl5NNUdyyMcDlNZVUvxnkqKS6so2lP5mQSyu6yq1aFJUpIDkaHbo5PFhMOHUNAn\n9jvdm7PmI2OMSQCfzxcZ2XbUkH4tLlMfClFavq/dGkdzd1x/DiMH53VovO0mBREZqKo7OnSvxhhj\nIgJ+f6Q2wMiWl6mpDbK7rIri0kqK9+wlPSPlM1dodYRYagqvi8gG3HMOnlTVug6PwhhjTJtSU5IY\nnJ/D4Hx3f0e8mtLa7fZW1cOA3wGfA1RE7mr2bARjjDE9REzXQqnqYuAq4Fe4p6c9ISLLvdFPjTHG\n9BDtJgUROVVE/gJ8iBvq+iJVHY57lvK/4hueMcaYzhRLn8IvgPuB76pq5DFnqlooIrfFLTJjjDGd\nLpbmo7Nwz0CoEpEhInKziGQAqOof4xueMcaYzhRLUvg7MMh7X+Gt89e4RWSMMSZhYmk+GqGq5wCo\najlwo4i8H9+wjDHGJEIsSSEsIhNUtRBARMYC7d6rICJ+YB4wCagB5qrqxqj5FwPXA2XAQ6p6vzf9\np8A5uGc3z2uYbowxJv5iSQrXAS+LyHbcs5b7AxfHsN55QJqqzvAuXb0ddzkrItIfuAWYDOwBXhGR\nRbh7+WYCxwIZ3r6NMcZ0knaTgqq+IiLDgQm4GoKqavuDjcNxwAveNpY0u+FtNLBSVUsARGQZMB1X\nqygEFgI5uJpEm3JzM0hKCsQQTteWn5+d6BC6DCuLpqw8mrLyaBSPsohl7CMBrgCycDWFgIiMUtXj\n21k1B9c01KBeRJJUNQhsAMaJSAGu8/oUYD2uFjIC+DwwCnhaRMaqaqsjoZaWVrU2q9vojiM/xouV\nRVNWHk0dx5uSAAAayUlEQVRZeTTqgIfstDg9lquP/olr4jkKeB8YAKyOYb1yIHqvfi8hoKqlwLXA\nAuARYAVQDOwGXlTVWlVVoBrIj2FfxhhjOkAsScGvqr/ENQWtwPUVTIthvTeBMwG8PoXChhkikoTr\nT5gFXAiM9ZZ/AzhdRHwiMhjIxCUKY4wxnSCWjuYqEUnFNe9MUdU3RCQthvUWArNF5C1cs9OlIjIH\ndyPcva5VihW42sDtqloMPCsixwPv4BLWlapav/9fyxhjzIFo98lrInIV7hLRrwJv4/oDAqp6WvzD\na589ea1nsbJoysqjKSuPRh3Qp9Dik9diaT56HfiiqhYBJwL3AucfcCTGGGO6rFiaj/6pqocDqOp2\nYHt8QzLGGJMosSSFtSLyC2ApsK9hoqq+HreojDHGJEQsSSEPOMn7axAGTo5LRMYYYxImljuaT2pv\nGWOMMT1DLHc0v4qrGTShqlZTMMaYHiaW5qNfRb1Pxg1qVxqXaIwxxiRULM1H/2026RURWYp7TKcx\nxpgeJJbmo+FRH33AOKBf3CIyxhiTMLE0H0XXFMJAEXB1fMIxxhiTSO3e0ayqo4DDvFcBTlbV5+Me\nmTHGmE7XblIQkQtwA9cBDAc+EJFz4xqVMcaYhIhl7KOfA6cCqOqHwBTgpngGZYwxJjFiSQopqrqz\n4YOq7sJ1OBtjjOlhYulofkNEHgH+7n2+CDeEtjHGmB4mlqRwJe5qo+8AdbirkebHMyhjjDGJEUvz\nUTKwT1XPxiWHfsSWTIwxxnQzsRzc/wGs8t5X4BLJX4EvtrWSiPiBecAkoAaYq6obo+ZfDFwPlAEP\nqer93vQVQLm32EeqemnM38YYY8xBiSUpjFDVcwBUtRy4UUTej2G984A0VZ0hItOB23HjJiEi/YFb\ngMnAHtzQGYuAHYBPVU/c729ijDHmoMWSFMIiMkFVCwFEZCyub6E9xwEvAKjqEhGZGjVvNLBSVUu8\nbS4DpgMfARki8pIX2w2quqStneTmZpCUFIghnK4tPz870SF0GVYWTVl5NGXl0SgeZRFLUrgOeFlE\nGh7DmQ98LYb1cnBNQw3qRSRJVYPABmCciBTgmqROAdYDVcBtwH3AGOB5ERFvnRaVllbFEErXZg8j\nb2Rl0ZSVR1NWHo0OtixaSyixDHPxCu5O5u8CTwOfALEMc1EORO/V33BwV9VS4FpgAfAI7o7pYlxi\n+JuqhlV1PbAbGBTDvowxxnSAWIa5GAXcDDwL/AyXEEbFsO03gTO9bUwHCqO2mYTrT5gFXAiM9Zb/\nJq7vAREZjKttfBrztzHGGHNQWm0+EpHzgctxB++FuCajP6vqzTFueyEwW0Tewt0BfamIzAGyVPVe\nEQFXQ6gGblfVYhG5H3hIRN7Ajcj6zbaajowxxnSstvoUFgCPAzMaLiUVkVCsG1bVEC6pRPsgav5N\nNBtDSVVrgTmx7sMYY0zHaispTAQuwQ1zsRnX9m83rRljTA/Wap+Cqq5W1euAIcBvgROBAhH5t4ic\n2UnxGWOM6USxPKO5HngKeEpE8oGLcUniuTjHZowxppPtV3OQqhYBf/D+jDHG9DCxDIhnjDGmCym7\n8Ro2XTE3Ltu2pGCMMSbCriYyxnQLZTdeQ0XAT/ZNdyQ6lE4RDoehrpZwTTXh6mrCNdXgvYZrawj5\n/YRDIXz+jj23t6RgTBfV2w6CLQnXVBPaU0poT4l77/dRu+xNCCSBP4AvEADvzxdIinofcMsEopdJ\narI8/gA+3/4/WbjsxmsA6HPrnU1jra+H2hrC1fsiB/HIX3U1RB3cwzX7CNe4ZfFewzU13vTG5QmH\nWy8boP6T7SQNHb7f36EtlhSMMZ0uHAoRriiLHPBDe0oJ7yklVFbSZBrV+5quB+x9cF7HBeIPtJNI\nGpNNw2uovAwIU/7rnzY9i6+rPbhYUlLxpabiS0vHn5WNLzUdvM++1LTGv7Q0ql9+Fl9SEoHBQzuk\nGKJZUjDGtKm1M+PWhKurmxzcw3tKGw/0Ze59uHwPhFofIMGXkYk/rx/+vnn4++Ti75tHzX9fwuf3\nk3b2Be6svL4e6oNR7+sJ1wch1PC+jWXqmy0TarZMsM6dqdfXE4raTvSZe6ikGFLT8GVm4e/XP3LA\n9qWmuelRn92BPRVS0yMH/s8svx/NQDWLF+EP+Du86QgsKRhjYhQOhQiXl0XO4kNlJYT37Gn37L6J\nQMAd5Ece4g74fXPx93Gvvj657nPfXHwpqZ9ZtXbpYvwBP6mzTonjt2xbOBSi7OffB6Dvr/83YXHE\nky/cRptVd1BUVNG9vwA2Rny0rlAW+3tmfLDC+6qoL95FaHeR+ysuIrR7F3VrCyEcxpedjRtTsuEl\n+r33xmsb97UwrcmMlqbha7qthhdvfmjXDgiF3bS2zu4zs/D3ycXXNzdydu/v631ueJ+ZfVBnt73x\n99GaDnieQosdKlZTMCbOwnV1hEqKCXkH/iYJYHcR4b2VLa/o84Hfjy8to2FL3kvUeVA4/NnOyHDY\nLRsOe2/DzdZv7X0r00Ih8PkItHR233DA75OLLyVlf4umW0p0Mog3Swqmy+iuV9uEQyHCe0qp372r\n8Uy/IQHs3kW4bE/LV5EkJ+PPyycw8hAC/fLx9x+Av98A/P3z8ffLp+I3N+AP+Mn+1W2d/6WiNJwZ\n51z3q4TGYTqHJQVj2hEOhwnvrYwc6Juc7Rfvch2O9fWfXdHnw5/bj8ChY70DvjvYB/q7A78vu09c\nOgqNORiWFEyXEKqsIFy9j/r6IJV/+oNrOvH5GtvQfVF/gM/nb3VeS599+MDvo7H9vIXP3jqhqr0Q\nDlP5pz9Ezvaprm4xbl9WDoFhI/H3H+DO9iNn/Pn48/q5yxiN6Ubi9osVET8wD5gE1ABzGx7W482/\nGLgeKAMeUtX7o+YNAJYDs1X1A0yPFKraS937y6hdvoSgrol0YtatWp7gyJy6VcshNc1r2smPatoZ\nQKB/Pv68fHxpaYkOM+56ehu6aSqepzHnAWmqOsN7RvPtwLkAItIfuAX3qM89wCsiskhVN4tIMnAP\n0MZ1baa7Cu+ronbVcuqWL6FuXWGk2SUwYjShXTvwp6eTdcNvGztQI52mNPsc9l68q2Ea2uxD0Z/D\n7qqZhvUJEw5FrQ/N9gOEQ1T+6Q7wQc6Nv8eXmXVAd712hD633tklrrYxvUs8k8JxwAsAqrpERKZG\nzRsNrFTVEgARWQZMBzYDtwF/An4ax9hMJwrXVFNX+B61y5dQt2YlBOsACAwdQcqU6SRPmU6g/wDK\nbrwGX8CPPyMzofH6kpMB8GdlJzQOYxIhnkkhB9c01KBeRJJUNQhsAMaJSAFQAZwCrBeRS4AiVX1R\nRGJKCrm5GSQlBTo49M6Xn9+zDkChmhr2vrecircWs3f5MsK1bgiAlGHDyZ55HNkzjiNlSNNb9PPv\neSARoX5GRcB1/naVf5OuEkdXYeXRKB5lEc+kUA5ER+z3EgKqWioi1wILgN3ACqAY+CEQFpFTgSOB\nh0XkHFXd0dpOSkur4hV/p+kpTQThujrq1hVSt/xtaletcMMEAP4BA0mdMp2UKdMJDB5GCO9soYXv\n3BXKIlTvmqASHQd0jfLoSqw8GnXAzWstTo9nUngTOBt4zOtTKGyYISJJuP6EWUAK8DJwg6o+FbXM\na8DlbSUEk3jh+iDBD9a4pqGV7xLe55K0v18+ySfMdolg6IiEtcsbY/ZPPJPCQmC2iLyFu+jvUhGZ\nA2Sp6r0iAq6GUA3crqrFcYzFtGN/bt0P19cT3LDOJYL3l0XuyPX1zSN15gmkTJlBYMTobpsI7Gob\n05vFLSmoagi4vNnkD6Lm3wTc1Mb6J8YnMnMgwqEQwQ+VuuVLqH3vHcIV5QD4cvqSeuJprkYwaozd\njGVMN2d31phWhcNh6j/aSO3yJdSuWEq4rBQAX1Y2KbNOIWXKdJIOHWuJwJgexJKCaSIcDlO/9SPX\nNLRiCaGS3YAb3z5l5okuERx2hHsIiTGmx7GkYNzYPsEg4doayn/1Q0JFO92MtDRSps1yiWDseHxJ\n9nMxpqez/+W9VLg+SHDDB9StWkFd4Qr3JCwgFA6RPHUGKZOnkzxuIr7k3jEcsjHGsaTQi4Sq9hJc\nu4raVcsJrlkZuXyUtHRIScGXkkqfW+9s8alXxpjewZJCD1e/u8jVBlYtJ7jhA/f8WsCf14+UaceR\nPHEKSYeOpfxXPwSwhGBML2dJoYcJh0LUb9scSQT1H2+NzAsMH0XyxCkkT5xMYMjwbnsfgTEmfiwp\nJFhHPG0sXFdLcP1aaletoG7VisiloyQlkXTERFImTiF5wlH4c/t1UNTGmJ7KkkI3FaqsoG71+9QV\nLqdu7SqoqQHcw9NTps0ieeJkkg+fgC8tPcGRGmO6E0sK3Uj9rh3UrVpO3aoVBD/UyDMA/PkFkWah\npNGHHdA9BDa0gzEGLCl0aeFQyN1RXOj6B0I7PnEzfD4CIw8lZdIUkidMxj9wsPUPGGM6hCWFLiZc\nW+OGn161grrV70XGGCI5JVIbSB5/FP6cPokN1BjTI1lS6ALCoXpq3nzVJYIPCqHOPZnMl9OHlGNP\nJHnCFJLHjrPLRY0xcWdJIYHqi3YSKi+DYB1Vf78PAP+goaRMnOwuGx1xiA02Z4zpVJYUEiAcDlO7\n5HWqHnvYPa84KZn0cy9yiSC/INHhGWN6MUsKnSy0t5KqRx6gbsVSSEvHl5lFICODtFPOSHRoxhiD\ntU10ojpdQ/mvf0LdiqUEDjmMnJ/9Fl9qWqLDMsaYiLjVFETED8wDJgE1wFxV3Rg1/2Lgetwz3B9S\n1ftFJAD8GRAgjHtG8+p4xdhZwsEg+55+jJpFz4HPR9rZXyLttHPsmQTGmC4nns1H5wFpqjpDRKYD\ntwPnAohIf+AWYDKwB3hFRBYBRwKo6rEiciLw64Z1uqv6Tz9m70PzqN+2GX9+AZmXXEHSqEMTHZYx\nxrQonknhOOAFAFVdIiJTo+aNBlaqagmAiCwDpqvqoyLyrLfMCFzC6JbC4TC1ixdRteDvUFdLyowT\nyLjg6/jSrLnIGNN1xTMp5OCahhrUi0iSqgaBDcA4ESkAKoBTgPUAqhoUkb8A5wNfam8nubkZJCV1\nrWaYYNkeds7/P6qWL8OfmUXB935A9vSZLS5bEXDdOvn52Z0ZYpdmZdGUlUdTVh6N4lEW8UwK5UB0\nxH4vIaCqpSJyLbAA2A2sAIobFlTVb4jIj4GlInKEqu5tbSelpVVxCf5A1a15n70P30O4opwkGUfm\n179DdW4/qosqWlw++6Y7yM/PpqiV+b2NlUVTVh5NWXk0OtiyaC2hxDMpvAmcDTzm9SkUNswQkSRc\nf8IsIAV4GbjB63weqqq/BaqAkPfX4cpuvAbouIHgwrW17HvyEWpeewkCAdK/MIfUk8+wm8+MMd1K\nPJPCQmC2iLwF+IBLRWQOkKWq94oIuBpCNXC7qhaLyBPAgyLyOpAMfF9V98Uxxg4R3L6FvQ/OI/Tp\ndvwDB5N56ZUkDRuZ6LCMMWa/xS0pqGoIuLzZ5A+i5t8E3NRsnb3AhfGKqaOFQyFqXn2RfU89CsEg\nqSfMJv38r9gYRcaYbsvuaD5AoT2l7H34TwQ/WI0vO4fMr11G8oSjEh2WMcYcFEsKB6D2/WVU/f0+\nwnsrSRp3JJkXX2ZDWRtjegRLCvshXF1N1YK/Uvvma5CcTPpFl5B6/Kn2gBtjTI9hSSFGwc0fsvfB\nuwkV7SQwdASZl15BYNDQRIdljDEdypJCO8KhENUvPUP1swsgVE/qqWeRfvYF+JKTEx2aMcZ0OEsK\nbajfXUTVX+YT3Kj4+uSS+Y3LSR47PtFhGWNM3FhSaEXtsreoevRBwvuqSD7qaDK+8i38WXZ7vTGm\nZ7Ok0Ex4XxVVjz5E7bI3ITWVjK99m5QZJ1hnsjGmV7CkECW4Udn70DxCJcUERh5C5iVXEBgwMNFh\nGWNMp7GkAITrg1Q/t5DqF54CIO2M80k78zx8ASseY0zv0uuPevW7driH4Gz+EH+/fDK/8V2SDpVE\nh2WMMQnRa5NCOByG2hrKf3sD1NSQcsyxZFx0Cb70jESHZowxCdN7k8K+Kqjehy89g4xLryTl6JYf\ngmOMMb1Jr00KACSnkH3Dbwj0y090JMYY0yX02qTgz8gEsIRgjDFR7LFgxhhjIiwpGGOMiYhb85GI\n+IF5wCSgBpirqhuj5l8MXA+UAQ+p6v0ikgw8AIwEUoFbVfXpeMVojDGmqXjWFM4D0lR1BvAT4PaG\nGSLSH7gFOBE4AfiqiIwEvgbsVtVZwOnAXXGMzxhjTDPxTArHAS8AqOoSYGrUvNHASlUt8Z7lvAyY\nDjwO/NxbxgcE4xifMcaYZuJ59VEOrmmoQb2IJKlqENgAjBORAqACOAVYr6qVACKSDfwLuLG9neTm\nZpCUFNjv4CoCLh/m53eNkU+7ShxdgZVFU1YeTVl5NIpHWcQzKZQD0RH7vYSAqpaKyLXAAmA3sAIo\nBhCRYcBCYJ6q/qO9nZSWVh1QcKH6EABFRRUHtH5Hys/P7hJxdAVWFk1ZeTRl5dHoYMuitYQSz+aj\nN4EzAURkOlDYMENEkoDJwCzgQmAs8KZXc3gJ+LGqPhDH2IwxxrQgnjWFhcBsEXkL1z9wqYjMAbJU\n9V4RAVdDqAZuV9ViEbkTyAV+LiINfQtnqOq+OMZpjDHGE7ek4HUgX95s8gdR828Cbmq2zjXANfGK\nyRhjTNvs5jVjjDERlhSMMcZEWFIwxhgTYUnBGGNMRK8dOrvPrXcmOgRjjOlyrKZgjDEmwpKCMcaY\nCEsKxhhjIiwpGGOMibCkYIwxJsKSgjHGmAhLCsYYYyIsKRhjjImwpGCMMSbCFw6HEx2DMcaYLsJq\nCsYYYyIsKRhjjImwpGCMMSbCkoIxxpgISwrGGGMiLCkYY4yJsKRgjDEmotc+ea2ziYgfmAdMAmqA\nuaq6MWr+V4DvA0GgELhCVUOJiLUztFceUcvdC5So6k86OcROFcPv42jgD4AP2AF8TVWrExFrvMVQ\nFl8FfgjUAw+o6vyEBNrJRGQa8HtVPbHZ9LOBX+COHQ+o6p8PZj9WU+g85wFpqjoD+Alwe8MMEUkH\nbgVOUtVjgT7A5xMSZedptTwaiMh3gAmdHViCtPX78AF/Bi5V1eOAF4ARCYmyc7T327gNOBU4Fvih\niOR2cnydTkR+BNwHpDWbngzcAZwGnABcJiIFB7MvSwqdp+E/M6q6BJgaNa8GmKmqVd7nJKBHngVG\naas8EJGZwDTgns4PLSHaKo/DgN3AtSLyXyBPVbXzQ+w0bf42gFW4E6c0XM2pNwzL8CHwhRamHw5s\nVNVSVa0F3gCOP5gdWVLoPDlAWdTnehFJAlDVkKruBBCRq4Es4OXOD7FTtVoeIjII+CVwVSICS5BW\nywPoD8wE7sKdIZ8iIid3cnydqa2yAFgNLAfWAM+q6p7ODC4RVHUBUNfCrOZlVYFLmAfMkkLnKQey\noz77VTXY8EFE/CJyGzAb+KKq9vSzn7bK4wLcgfA5XPPBHBG5pHPD63Rtlcdu3NngOlWtw51FNz97\n7klaLQsRmQicBYwCRgIDROSCTo+w62heVtnAQSVJSwqd503gTAARmY7rTI52D646fF5UM1JP1mp5\nqOr/quoUr0Ptd8A/VPWhRATZidr6fWwCskTkUO/zLNxZck/VVlmUAfuAfapaD+wCenyfQhvWAWNE\nJE9EUnBNR28fzAZtlNROEnVFxURcO+ilwGRcU9G73t9iGttH71TVhQkItVO0VR6qem/UcpcAY3vR\n1UctlofXXPQ7b95bqnpNwoKNsxjK4nLgm0Atrq392157eo8mIiOBR1V1uojMobE8Gq4+8uOuPrr7\nYPZjScEYY0yENR8ZY4yJsKRgjDEmwpKCMcaYCEsKxhhjIiwpGGOMibAB8Uy35F2etx5Y601Kxw1/\ncFXD3eFtrPuqqp60H/t6DRgKVOIukfQDt6jqY/sf+cERkZuAV1R1cWfv2/QOVlMw3dknqnqkqh4J\njAU2Av+KYb0TD2Bfc719TQIuAh4UkQEHsJ2DdQIQSMB+TS9hNQXTI6hqWER+Cez0hkJYC8wHxgMF\ngOIGFPs9gIgsVdVpInIVcDGQCYSAi1R1XTv7WisilcAIEakC7vb2E8ANbfyId9PdN3DDdTyDu2P9\nQWAAUIVLMqtE5Ou4IdP9uPF8rlTVahH5FJfgjsMNiXwh7k7mqcB9InI+kAf8GsjA3dX7I1V9XESG\nAn/3phUCJ6jqUBHJainWAylv03NZTcH0GN5drRtwtYaZQK03/PKhuOalM1X1e96y00QkBzdM84mq\nOh54Eriivf2IyOdwB9V1wI3AclWdghti4GciMtpbdChwlKregLtDd4G3n18BN4rIOODbuBFyj8QN\n2XCdt+5AYJGqHgW8jmsWexh35/tcVS0ErvbeTwa+hburFeBO4J+qOhGXWIZ409uK1RjAagqm5wnj\nxsV5XUR2i8iVuCQxBjekSISqlnvDBXxZRA4DTgfeb2W793m1gySgBLhQVStF5FQgQ0S+6S2XCYzz\n3q+IGtTuBOAr3n6fA57zailjgCUiApACrIja5wve62paHg75a8DnvQHhpkd9v9nAJd6+FopIwwBp\nrcW6qZXvbHohSwqmx/AGBBNgrYicA9yMO2t+ENeM42u2/DDgNdyQ1M/jnmh2VCubn6uqr7UwPYB7\nCtoKb5sFuKTxVdzAbQ0iwx57D8053Fv3sYbai9e8E/k/GfVktXDz2D2LgVe977AI+Ic3vZ6WWwFa\ni9WYCGs+Mj2CN4jaTcASVf0Qd1b8mKo+iDvYH09jB23D+PxH44akvgNYCpzB/nfi/gf4rhfDINwV\nUMNbWO514Mve+1OBe3EH8/NFZICXKObj+hfaEgSSRCQP9/CdX3g1j9OiYn8ZmOPFdAbQdz9jNb2Y\nJQXTnQ0WkfdF5H1gJa7tfI4378/AV0TkPeAJYAluDH6Ap7zlXwL8IrLWm785aplY3QSki8hq3EH3\nR15Sau4q4IterDcBl6nqSu/9f3BDYftxI6G25QXgT7gmsfuANd53HIBrGsrEJZYvetMvonF8/Vhj\nNb2YjZJqTA8jIt/D3cuwVkQmA3/2OpeNaZf1KRjT82wAHhGREO5Z399OcDymG7GagjHGmAjrUzDG\nGBNhScEYY0yEJQVjjDERlhSMMcZEWFIwxhgT8f8BEgZGCaVLPZ0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132720dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataSizePlot(np.linspace(0.1, 1, 10, endpoint=True),train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data size doesnt affect training data as much since the model is able to fit the data well. However, small data size does not allow the model to generalize well to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Performance vs. Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show the vectors representing the binary and TF encodings of one of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 20_newsgroups/alt.atheism/51060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  0,  1,  6, 10,  0,  0,  0,  0,  6, 49,  0,  0,  0,  0,\n",
       "        0, 25,  0,  0,  0,  2,  0,  0,  2,  0,  0,  4,  0,  1,  0,  4,  2,\n",
       "        0,  1,  0,  0,  0,  2,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,\n",
       "        5,  0,  0,  0, 18,  3,  6,  4,  5,  0,  1,  0,  0,  0,  0,  2,  0,\n",
       "        0,  1,  1,  0,  0,  0,  0,  0,  0,  1,  0,  7,  0,  3,  0,  0,  0,\n",
       "        5,  0,  0,  1,  0, 11,  0,  0,  1,  0,  0, 26,  5, 19,  0,  0,  0,\n",
       "        0,  0,  6,  0,  1,  0,  8,  7,  0, 14,  1,  0,  3,  0,  0, 17,  0,\n",
       "        0,  0, 21,  8,  0,  0,  1,  0,  2,  0,  2,  1,  0,  0,  2,  0,  1,\n",
       "        0, 23, 42,  0,  1,  0,  0,  0,  0, 11,  0,  4,  0,  0,  6,  0,  0,\n",
       "        1,  0,  2,  5,  0,  0,  0,  0,  0,  0,  0,  3,  9,  0,  0,  0,  0,\n",
       "        1,  0,  2,  4,  2,  0, 10,  5,  0,  0,  0,  1,  0,  6,  0,  0,  0,\n",
       "        0,  5,  2,  0,  0,  0,  0,  3,  5,  7,  0,  0,  2])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.loc[1]\n",
    "#print('ss')\n",
    "\n",
    "print('File:', data.loc[1]['label'])\n",
    "data.drop(['label', 'target'], axis=1).loc[1].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 20_newsgroups/alt.atheism/51060\n",
      "[0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0\n",
      " 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('File:', data.loc[1]['label'])\n",
    "features_mod = data.drop(['label', 'target'], axis=1).loc[1].as_matrix()\n",
    "\n",
    "for i in range(len(features_mod)):\n",
    "    features_mod[i] = 1 if features_mod[i] > 0 else 0\n",
    "        \n",
    "print(features_mod)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Run enough trials to obtain non-overlapping 95% CIs on the average accuracy of each feature encoding method (binary and TF). Which feature encoding method performs better on this dataset? Why do you think this occurs? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get this answer easily by running dataPreprocessor fuction and randomSplitCI function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TF does a lot better than Boolean encoding. This makes sense as it weights more frequent terms (likely to have more discriminating power) more highly.\n",
    "\n",
    "\n",
    "For TF\n",
    "Train    \n",
    "Result of trails:[0.96892191183825105, 0.97006501393155675, 0.96970779452739875, 0.97056512109737803, 0.9691362434807459, 0.97020790169322002, 0.96992212616989359, 0.96970779452739875, 0.97085089662070445, 0.96935057512324074]     \n",
    "Average Accuracy: 0.9698435379009787     \n",
    "Confidence Interval: 0.0004366326298835821\n",
    "\n",
    "Test    \n",
    "Result of trails:[0.95350000000000001, 0.94783333333333331, 0.95083333333333331, 0.94950000000000001, 0.95399999999999996, 0.94933333333333336, 0.95299999999999996, 0.95233333333333337, 0.94799999999999995, 0.95133333333333336]     \n",
    "Average Accuracy: 0.9509666666666667     \n",
    "Confidence Interval: 0.0016033343964252378\n",
    "\n",
    "BLN\n",
    "Train    \n",
    "Result of trails:[0.96527827391583909, 0.96434950346502823, 0.96520683003500751, 0.96384939629920696, 0.96484961063084951, 0.96277773808673284, 0.96420661570336497, 0.96320640137172253, 0.96320640137172253, 0.96313495749089095]     \n",
    "Average Accuracy: 0.9640065728370365     \n",
    "Confidence Interval: 0.0006530796155977958\n",
    "\n",
    "Test    \n",
    "Result of trails:[0.94733333333333336, 0.94933333333333336, 0.94416666666666671, 0.94316666666666671, 0.94699999999999995, 0.94850000000000001, 0.94833333333333336, 0.94516666666666671, 0.94550000000000001, 0.94899999999999995]     \n",
    "Average Accuracy: 0.94675     \n",
    "Confidence Interval: 0.001532260922333717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Comparison vs. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Run enough trials to obtain non-overlapping 95% CIs on the average accuracy of logistic regression vs. Naive Bayes classifiers.  Which classification algorithm performs better on this dataset? Why do you think this occurs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LR does better than NB. Logistic Regression directly estimates the parameters of P(Y|X) (discriminative classifier) whereas Naive Bayes directly estimates parameters for P(Y) and P(X|Y) (generative).\n",
    "\n",
    "Logistic Regression is a linear classifier over X. The linear classifiers produced by Logistic Regression and Gaussian Naive Bayes are identical provided the Naive Bayes assumptions hold. However, if these assumptions do not hold, the Naive Bayes bias will cause it to perform less accurately than LR, in the limit. Put another way, Naive Bayes is a learning algorithm with greater bias, but lower variance, than Logistic Regression. If this bias is appropriate given the actual data, Naive Bayes will be preferred. Otherwise, Logistic Regression will be preferred.\n",
    "\n",
    "### LR\n",
    "**Train**    \n",
    "- Result of trails:[0.96527827391583909, 0.96434950346502823, 0.96520683003500751, 0.96384939629920696, 0.96484961063084951, 0.96277773808673284, 0.96420661570336497, 0.96320640137172253, 0.96320640137172253, 0.96313495749089095]     \n",
    "- Average Accuracy: 0.9640065728370365     \n",
    "- Confidence Interval: 0.0006530796155977958\n",
    "\n",
    "**Test**    \n",
    "- Result of trails:[0.94733333333333336, 0.94933333333333336, 0.94416666666666671, 0.94316666666666671, 0.94699999999999995, 0.94850000000000001, 0.94833333333333336, 0.94516666666666671, 0.94550000000000001, 0.94899999999999995]     \n",
    "- Average Accuracy: 0.94675     \n",
    "- Confidence Interval: 0.001532260922333717\n",
    "\n",
    "\n",
    "### NB\n",
    "**Train**    \n",
    "- Result of trails:[0.59191255268986209, 0.60398656855040367, 0.57748088876187753, 0.56726441380295778, 0.56554976066299922, 0.58712581267414443, 0.58598271058083873, 0.58784025148246055, 0.5671929699221262, 0.55740515824819603]     \n",
    "- Average Accuracy: 0.5791741087375866     \n",
    "- Confidence Interval: 0.010422248985307192\n",
    "\n",
    "**Test**    \n",
    "- Result of trails:[0.5665, 0.5718333333333333, 0.55000000000000004, 0.54149999999999998, 0.54216666666666669, 0.5658333333333333, 0.55233333333333334, 0.55649999999999999, 0.53766666666666663, 0.52449999999999997]     \n",
    "- Average Accuracy: 0.5508833333333334     \n",
    "- Confidence Interval: 0.010598400861842161\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilesDirectory(datapath='20_newsgroups'):\n",
    "    # create file directory for all files\n",
    "    files = []\n",
    "    for (path, dirnames, filenames) in os.walk(datapath):\n",
    "        files.extend(os.path.join(path, name) for name in filenames)\n",
    "    # putting file directories into pandas dataframe\n",
    "    directorydf= pd.DataFrame(files)\n",
    "    directorydf.columns = ['Directories']\n",
    "    return directorydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directorydf = getFilesDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create function to help encoding the targets\n",
    "def label_target (d):\n",
    "    if d.find(\"alt.atheism\") > 0 :\n",
    "      return 0\n",
    "    if d.find(\"comp.graphics\") > 0:\n",
    "      return 1\n",
    "    if d.find(\"comp.os.ms-windows.misc\") > 0:\n",
    "      return 2\n",
    "    if d.find(\"comp.sys.ibm.pc.hardware\") > 0:\n",
    "      return 3\n",
    "    if d.find(\"comp.sys.mac.hardware\") > 0:\n",
    "      return 4\n",
    "    if d.find(\"comp.windows.x\") > 0:\n",
    "      return 5\n",
    "    if d.find(\"misc.forsale\") > 0:\n",
    "      return 6\n",
    "    if d.find(\"rec.autos\") > 0:\n",
    "      return 7\n",
    "    if d.find(\"rec.motorcycles\") > 0:\n",
    "      return 8\n",
    "    if d.find(\"rec.sport.baseball\") > 0:\n",
    "      return 9\n",
    "    if d.find(\"rec.sport.hockey\") > 0:\n",
    "      return 10\n",
    "    if d.find(\"sci.crypt\") > 0:\n",
    "      return 11\n",
    "    if d.find(\"sci.electronics\") > 0:\n",
    "      return 12\n",
    "    if d.find(\"sci.med\") > 0:\n",
    "      return 13\n",
    "    if d.find(\"sci.space\") > 0:\n",
    "      return 14\n",
    "    if d.find(\"soc.religion.christian\") > 0:\n",
    "      return 15\n",
    "    if d.find(\"talk.politics.guns\") > 0:\n",
    "      return 16\n",
    "    if d.find(\"talk.politics.mideast\") > 0:\n",
    "      return 17\n",
    "    if d.find(\"talk.politics.misc\") > 0:\n",
    "      return 18\n",
    "    if d.find(\"talk.religion.misc\") > 0:\n",
    "      return 19\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataPreprocessor\n",
    "assert(isinstance(data, pd.DataFrame)), \"data should be pandas dataframe type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomSplitCI\n",
    "assert(isinstance(train_scores, list)), \"train_scores should be list type\"\n",
    "assert(isinstance(test_scores, list)), \"test_scores should be list type\"\n",
    "assert(isinstance(train_mean, float)), \"train_mean should be float type\"\n",
    "assert(isinstance(test_mean, float)), \"test_mean should be float type\"\n",
    "assert(isinstance(train_ci, float)), \"train_ci should be float type\"\n",
    "assert(isinstance(test_ci, float)), \"test_ci should be float type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomSplitCM\n",
    "assert(isinstance(cm, pd.DataFrame)), \"Confusion matrix should be pandas dataframe type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#featureSizeAC\n",
    "assert(isinstance(train_mean_fs, list)), \"train_mean_fs should be list type\"\n",
    "assert(isinstance(test_mean_fs, list)), \"test_mean_fs should be list type\"\n",
    "assert(isinstance(train_ci_fs, list)), \"train_ci_fs should be list type\"\n",
    "assert(isinstance(test_ci_fs, list)), \"test_ci_fs should be list type\"\n",
    "assert(len(train_mean_fs)==len(train_ci_fs)), \\\n",
    "\"list length of mean and confidence interval are not equal(train)\"\n",
    "assert(len(test_mean_fs)==len(test_ci_fs)), \\\n",
    "\"list length of mean and confidence interval are not equal(test)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperParameterAC\n",
    "assert(isinstance(train_mean_hp, list)), \"train_mean_hp should be list type\"\n",
    "assert(isinstance(test_mean_hp, list)), \"test_mean_hp should be list type\"\n",
    "assert(isinstance(train_ci_hp, list)), \"train_ci_hp should be list type\"\n",
    "assert(isinstance(test_ci_hp, list)), \"test_ci_hp should be list type\"\n",
    "assert(len(train_mean_hp)==len(train_ci_hp)), \\\n",
    "\"list length of mean and confidence interval are not equal(train)\"\n",
    "assert(len(test_mean_hp)==len(test_ci_hp)), \\\n",
    "\"list length of mean and confidence interval are not equal(test)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataSizeAC\n",
    "train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds\n",
    "assert(isinstance(train_mean_ds, list)), \"train_mean_ds should be list type\"\n",
    "assert(isinstance(test_mean_ds, list)), \"test_mean_ds should be list type\"\n",
    "assert(isinstance(train_ci_ds, list)), \"train_ci_ds should be list type\"\n",
    "assert(isinstance(test_ci_ds, list)), \"test_ci_ds should be list type\"\n",
    "assert(len(train_mean_ds)==len(train_ci_ds)), \\\n",
    "\"list length of mean and confidence interval are not equal(train)\"\n",
    "assert(len(test_mean_ds)==len(test_ci_ds)), \\\n",
    "\"list length of mean and confidence interval are not equal(test)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
